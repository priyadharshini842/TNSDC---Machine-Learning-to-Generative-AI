{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import secrets\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the train,test and model directories\n",
    "\n",
    "We will create the directories for train,test and model training paths if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = os.path.join(os.getcwd(),\"imgs\",\"test\")\n",
    "TRAIN_DIR = os.path.join(os.getcwd(),\"imgs\",\"train\")\n",
    "MODEL_PATH = os.path.join(os.getcwd(),\"model\",\"self_trained\")\n",
    "PICKLE_DIR = os.path.join(os.getcwd(),\"pickle_files\")\n",
    "CSV_DIR = os.path.join(os.getcwd(),\"csv_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TEST_DIR):\n",
    "    print(\"Testing data does not exists\")\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    print(\"Training data does not exists\")\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Model path does not exists\")\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    print(\"Model path created\")\n",
    "if not os.path.exists(PICKLE_DIR):\n",
    "    os.makedirs(PICKLE_DIR)\n",
    "if not os.path.exists(CSV_DIR):\n",
    "    os.makedirs(CSV_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the data augmentation definition\n",
    "\n",
    "gen_per_image = 1\n",
    "gen_per_class = 200\n",
    "rotation_range = 5\n",
    "width_shift_range = 0.02\n",
    "height_shift_range = 0.02\n",
    "shear_range = 0.01\n",
    "zoom_range = 0.05\n",
    "horizontal_flip = False\n",
    "fill_mode = \"nearest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "height_shift_range = 0.02\n",
    "shear_range = 0.01\n",
    "zoom_range = 0.05\n",
    "horizontal_flip = False\n",
    "fill_mode = \"nearest\"\n",
    "\n",
    "def increase_brightness(img, value):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    return img\n",
    "\n",
    "def change_contrast(img, level):\n",
    "    img = Image.fromarray(img.astype('uint8'))\n",
    "    factor = (259 * (level + 255)) / (255 * (259 - level))\n",
    "    def contrast(c):\n",
    "        return 128 + factor * (c - 128)\n",
    "    return np.array(img.point(contrast))\n",
    "\n",
    "def pad_img(img):\n",
    "    h, w = img.shape[:2]\n",
    "    new_h = int((5 + secrets.randbelow(16)) * h / 100) + h\n",
    "    new_w = int((5 + secrets.randbelow(16)) * w / 100) + w\n",
    "\n",
    "    full_sheet = np.ones((new_h, new_w, 3)) * 255\n",
    "\n",
    "    p_X = secrets.randbelow(new_h - img.shape[0])\n",
    "    p_Y = secrets.randbelow(new_w - img.shape[1])\n",
    "\n",
    "    full_sheet[p_X : p_X + img.shape[0], p_Y : p_Y + img.shape[1]] = img\n",
    "\n",
    "    full_sheet = cv2.resize(full_sheet, (w, h), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    return full_sheet.astype(np.uint8)\n",
    "\n",
    "def preprocess_img(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    x = secrets.randbelow(2)\n",
    "\n",
    "    if x == 0:\n",
    "        # img = pad_img(img)\n",
    "        img = increase_brightness(img, secrets.randbelow(26))\n",
    "        img = change_contrast(img, secrets.randbelow(51))\n",
    "    else:\n",
    "        # img = pad_img(img)\n",
    "        img = change_contrast(img, secrets.randbelow(51))\n",
    "        img = increase_brightness(img, secrets.randbelow(26))\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 224\n",
    "NUM_EPOCH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "#Initialise the parameters for Augmentation.\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range = rotation_range,\n",
    "        width_shift_range = width_shift_range,\n",
    "        height_shift_range = height_shift_range,\n",
    "        shear_range = shear_range,\n",
    "        zoom_range = zoom_range,\n",
    "        horizontal_flip = horizontal_flip,\n",
    "        fill_mode = fill_mode,\n",
    "        validation_split = 0.2,\n",
    "        preprocessing_function = preprocess_img)\n",
    "\n",
    "\n",
    "train_data = datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        subset='training',shuffle=False)\n",
    "\n",
    "valid_data = datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        subset='validation',shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 64)      832       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 112, 112, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 128)     32896     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 256)       131328    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 28, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 512)       524800    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100352)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               50176500  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,871,366\n",
      "Trainable params: 50,871,366\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(IMAGE_SIZE,IMAGE_SIZE,3), kernel_initializer='glorot_normal'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=256, kernel_size=2, padding='same', activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax', kernel_initializer='glorot_normal'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(MODEL_PATH,\"distracted-{epoch:02d}-{val_accuracy:.2f}.hdf5\")\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 19:18:35.626726: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - ETA: 0s - loss: 222.4196 - accuracy: 0.0765\n",
      "Epoch 1: val_accuracy improved from -inf to 0.12944, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-01-0.13.hdf5\n",
      "141/141 [==============================] - 693s 5s/step - loss: 222.4196 - accuracy: 0.0765 - val_loss: 2.2842 - val_accuracy: 0.1294\n",
      "Epoch 2/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 4.4255 - accuracy: 0.1540\n",
      "Epoch 2: val_accuracy did not improve from 0.12944\n",
      "141/141 [==============================] - 995s 7s/step - loss: 4.4255 - accuracy: 0.1540 - val_loss: 3.4919 - val_accuracy: 0.1038\n",
      "Epoch 3/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 2.8475 - accuracy: 0.2540\n",
      "Epoch 3: val_accuracy improved from 0.12944 to 0.34814, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-03-0.35.hdf5\n",
      "141/141 [==============================] - 965s 7s/step - loss: 2.8475 - accuracy: 0.2540 - val_loss: 1.8061 - val_accuracy: 0.3481\n",
      "Epoch 4/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.9625 - accuracy: 0.4178\n",
      "Epoch 4: val_accuracy improved from 0.34814 to 0.65722, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-04-0.66.hdf5\n",
      "141/141 [==============================] - 1086s 8s/step - loss: 1.9625 - accuracy: 0.4178 - val_loss: 1.1108 - val_accuracy: 0.6572\n",
      "Epoch 5/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.2395 - accuracy: 0.5891\n",
      "Epoch 5: val_accuracy improved from 0.65722 to 0.79089, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-05-0.79.hdf5\n",
      "141/141 [==============================] - 897s 6s/step - loss: 1.2395 - accuracy: 0.5891 - val_loss: 0.6744 - val_accuracy: 0.7909\n",
      "Epoch 6/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.8330 - accuracy: 0.7272\n",
      "Epoch 6: val_accuracy improved from 0.79089 to 0.85561, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-06-0.86.hdf5\n",
      "141/141 [==============================] - 918s 7s/step - loss: 0.8330 - accuracy: 0.7272 - val_loss: 0.4268 - val_accuracy: 0.8556\n",
      "Epoch 7/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.5866 - accuracy: 0.8115\n",
      "Epoch 7: val_accuracy improved from 0.85561 to 0.92189, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-07-0.92.hdf5\n",
      "141/141 [==============================] - 981s 7s/step - loss: 0.5866 - accuracy: 0.8115 - val_loss: 0.2408 - val_accuracy: 0.9219\n",
      "Epoch 8/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.4068 - accuracy: 0.8726\n",
      "Epoch 8: val_accuracy improved from 0.92189 to 0.94510, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-08-0.95.hdf5\n",
      "141/141 [==============================] - 948s 7s/step - loss: 0.4068 - accuracy: 0.8726 - val_loss: 0.1712 - val_accuracy: 0.9451\n",
      "Epoch 9/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.3579 - accuracy: 0.8936\n",
      "Epoch 9: val_accuracy improved from 0.94510 to 0.96429, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-09-0.96.hdf5\n",
      "141/141 [==============================] - 934s 7s/step - loss: 0.3579 - accuracy: 0.8936 - val_loss: 0.1163 - val_accuracy: 0.9643\n",
      "Epoch 10/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9189\n",
      "Epoch 10: val_accuracy did not improve from 0.96429\n",
      "141/141 [==============================] - 974s 7s/step - loss: 0.2639 - accuracy: 0.9189 - val_loss: 0.1305 - val_accuracy: 0.9632\n",
      "Epoch 11/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9317\n",
      "Epoch 11: val_accuracy improved from 0.96429 to 0.97947, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-11-0.98.hdf5\n",
      "141/141 [==============================] - 963s 7s/step - loss: 0.2297 - accuracy: 0.9317 - val_loss: 0.0753 - val_accuracy: 0.9795\n",
      "Epoch 12/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.9432\n",
      "Epoch 12: val_accuracy did not improve from 0.97947\n",
      "141/141 [==============================] - 1219s 9s/step - loss: 0.1904 - accuracy: 0.9432 - val_loss: 0.1008 - val_accuracy: 0.9730\n",
      "Epoch 13/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9488\n",
      "Epoch 13: val_accuracy improved from 0.97947 to 0.98326, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-13-0.98.hdf5\n",
      "141/141 [==============================] - 1062s 8s/step - loss: 0.1770 - accuracy: 0.9488 - val_loss: 0.0608 - val_accuracy: 0.9833\n",
      "Epoch 14/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9507\n",
      "Epoch 14: val_accuracy did not improve from 0.98326\n",
      "141/141 [==============================] - 1039s 7s/step - loss: 0.1799 - accuracy: 0.9507 - val_loss: 0.0818 - val_accuracy: 0.9826\n",
      "Epoch 15/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.9587\n",
      "Epoch 15: val_accuracy did not improve from 0.98326\n",
      "141/141 [==============================] - 1174s 8s/step - loss: 0.1458 - accuracy: 0.9587 - val_loss: 0.0668 - val_accuracy: 0.9824\n",
      "Epoch 16/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.9596\n",
      "Epoch 16: val_accuracy did not improve from 0.98326\n",
      "141/141 [==============================] - 1061s 8s/step - loss: 0.1458 - accuracy: 0.9596 - val_loss: 0.0674 - val_accuracy: 0.9804\n",
      "Epoch 17/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9605\n",
      "Epoch 17: val_accuracy improved from 0.98326 to 0.98862, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-17-0.99.hdf5\n",
      "141/141 [==============================] - 1005s 7s/step - loss: 0.1390 - accuracy: 0.9605 - val_loss: 0.0445 - val_accuracy: 0.9886\n",
      "Epoch 18/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9580\n",
      "Epoch 18: val_accuracy improved from 0.98862 to 0.98951, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-18-0.99.hdf5\n",
      "141/141 [==============================] - 1082s 8s/step - loss: 0.1453 - accuracy: 0.9580 - val_loss: 0.0318 - val_accuracy: 0.9895\n",
      "Epoch 19/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9602\n",
      "Epoch 19: val_accuracy did not improve from 0.98951\n",
      "141/141 [==============================] - 1013s 7s/step - loss: 0.1461 - accuracy: 0.9602 - val_loss: 0.0448 - val_accuracy: 0.9888\n",
      "Epoch 20/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9667\n",
      "Epoch 20: val_accuracy did not improve from 0.98951\n",
      "141/141 [==============================] - 931s 7s/step - loss: 0.1256 - accuracy: 0.9667 - val_loss: 0.0613 - val_accuracy: 0.9855\n",
      "Epoch 21/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9654\n",
      "Epoch 21: val_accuracy did not improve from 0.98951\n",
      "141/141 [==============================] - 955s 7s/step - loss: 0.1317 - accuracy: 0.9654 - val_loss: 0.0484 - val_accuracy: 0.9873\n",
      "Epoch 22/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1382 - accuracy: 0.9649\n",
      "Epoch 22: val_accuracy did not improve from 0.98951\n",
      "141/141 [==============================] - 933s 7s/step - loss: 0.1382 - accuracy: 0.9649 - val_loss: 0.0571 - val_accuracy: 0.9868\n",
      "Epoch 23/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9672\n",
      "Epoch 23: val_accuracy improved from 0.98951 to 0.99264, saving model to /Users/abhjha8/ml_projects/DistractedDriverDetection/model/self_trained/distracted-23-0.99.hdf5\n",
      "141/141 [==============================] - 929s 7s/step - loss: 0.1265 - accuracy: 0.9672 - val_loss: 0.0273 - val_accuracy: 0.9926\n",
      "Epoch 24/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9702\n",
      "Epoch 24: val_accuracy did not improve from 0.99264\n",
      "141/141 [==============================] - 959s 7s/step - loss: 0.1166 - accuracy: 0.9702 - val_loss: 0.0386 - val_accuracy: 0.9902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9681\n",
      "Epoch 25: val_accuracy did not improve from 0.99264\n",
      "141/141 [==============================] - 936s 7s/step - loss: 0.1235 - accuracy: 0.9681 - val_loss: 0.0692 - val_accuracy: 0.9868\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(train_data,validation_data = valid_data,epochs=NUM_EPOCH,shuffle=True,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "\n",
      "text/plain": [
       "<Figure size 1200x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "ax1.plot(model_history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax1.plot(model_history.history['val_loss'], color='r', label=\"validation loss\")\n",
    "ax1.set_xticks(np.arange(1, 25, 1))\n",
    "ax1.set_yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "ax2.plot(model_history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax2.plot(model_history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "ax2.set_xticks(np.arange(1, 25, 1))\n",
    "\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "\n",
    "Finding the Confusion matrix,Precision,Recall and F1 score to analyse the model thus created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    fig.savefig(os.path.join(MODEL_PATH,\"confusion_matrix.png\"))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_heatmap(n_labels, n_predictions, class_names):\n",
    "    labels = n_labels #sess.run(tf.argmax(n_labels, 1))\n",
    "    predictions = n_predictions #sess.run(tf.argmax(n_predictions, 1))\n",
    "\n",
    "#     confusion_matrix = sess.run(tf.contrib.metrics.confusion_matrix(labels, predictions))\n",
    "    matrix = confusion_matrix(labels,predictions.argmax(axis=1))\n",
    "    row_sum = np.sum(matrix, axis = 1)\n",
    "    w, h = matrix.shape\n",
    "\n",
    "    c_m = np.zeros((w, h))\n",
    "\n",
    "    for i in range(h):\n",
    "        c_m[i] = matrix[i] * 100 / row_sum[i]\n",
    "\n",
    "    c = c_m.astype(dtype = np.uint8)\n",
    "\n",
    "    \n",
    "    heatmap = print_confusion_matrix(c, class_names, figsize=(18,10), fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 82s 2s/step\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(valid_data)\n",
    "\n",
    "valid_list = valid_data.classes.tolist()\n",
    "\n",
    "ypred_class = np.argmax(ypred,axis=1)\n",
    "ytest = valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n"
     ]
    }
   ],
   "source": [
    "class_names = list()\n",
    "for name,idx in valid_data.class_indices.items():\n",
    "    class_names.append(name)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "\n",
      "text/plain": [
       "<Figure size 1800x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_heatmap(ytest,ypred,class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.985941\n",
      "Precision: 0.986327\n",
      "Recall: 0.985941\n",
      "F1 score: 0.985974\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(ytest,ypred_class)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(ytest, ypred_class,average='weighted')\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(ytest,ypred_class,average='weighted')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(ytest,ypred_class,average='weighted')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 82s 2s/step - loss: 0.0628 - accuracy: 0.9862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0627790167927742, 0.9861637949943542]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
